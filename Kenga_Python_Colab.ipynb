{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import torch\n",
    "\n",
    "# Импорт зависимостей из проекта\n",
    "from src.neural.models.kenga import KengaConfig, KengaModel\n",
    "from src.neural.training import KengaTrainer\n",
    "from src.neural.data import TextDataset\n",
    "from src.utils.tokenizer import Tokenizer\n",
    "\n",
    "# Создаем необходимые директории\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Создаем файлы с обучающими данными, если их нет\n",
    "if not os.path.exists(\"data/train.txt\"):\n",
    "    with open(\"data/train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"This is a sample sentence.\\nThis is another sentence.\\n\")\n",
    "\n",
    "if not os.path.exists(\"data/val.txt\"):\n",
    "    with open(\"data/val.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"This is a sample sentence.\\nThis is another sentence.\\n\")\n",
    "\n",
    "# Инициализируем токенизатор и обучаем его\n",
    "tokenizer = Tokenizer()\n",
    "train_files = [\"data/train.txt\"]\n",
    "val_files = [\"data/val.txt\"]\n",
    "tokenizer.train(train_files)\n",
    "tokenizer.save(\"models\", \"kenga\")\n",
    "\n",
    "def generate_tokens(files):\n",
    "    tokens = []\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    t = tokenizer.encode(line)\n",
    "                    if t:\n",
    "                        tokens.append(t)\n",
    "    return tokens\n",
    "\n",
    "train_tokens = generate_tokens(train_files)\n",
    "val_tokens = generate_tokens(val_files)\n",
    "\n",
    "train_dataset = TextDataset(train_tokens)\n",
    "val_dataset = TextDataset(val_tokens)\n",
    "\n",
    "# Конфигурация модели и создание модели\n",
    "config = KengaConfig()\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "model = KengaModel(config)\n",
    "\n",
    "train_config = {\n",
    "    \"lr\": 1e-4,\n",
    "    \"batch_size\": 4,\n",
    "    \"epochs\": 5,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"model_path\": \"models/kenga.pth\"\n",
    "}\n",
    "\n",
    "# Создаем объект тренера и запускаем обучение\n",
    "trainer = KengaTrainer(model, train_dataset, val_dataset, train_config)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "push-to-gitlab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если вы настроили git-креденциалы в Colab, можно автоматически отправить чекпоинт в репозиторий\n",
    "!git config --global user.email \"your_email@example.com\"\n",
    "!git config --global user.name \"Your Name\"\n",
    "!git add models/kenga.pth\n",
    "!git commit -m \"Обновленный чекпоинт после обучения в Colab\"\n",
    "!git push origin master\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
